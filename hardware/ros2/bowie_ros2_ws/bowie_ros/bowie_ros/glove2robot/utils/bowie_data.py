import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from tabulate import tabulate
import plotly.graph_objects as go
import torch
from scipy.signal import savgol_filter
import pytorch_kinematics as pk  # https://github.com/UM-ARM-Lab/pytorch_kinematics

from gum.labs.kinematics.franka_kinematics import FrankaKinematics
from gum.labs.kinematics.metahand_kinematics import DifferentiableMetahandKinematics
from utils.constants import HAND_KEYPOINTS, ALLEGRO_JOINTS

class BowieSyncExtractedData:

    def __init__(self, cfg):

        # please set at minimum, the paths config
        self.cfg = cfg
        self.verbose = self.cfg.verbose
        self.window_length = self.cfg.window_length
        self.polyorder = self.cfg.polyorder
        # these keys should be generated by the `extract_sync_visual.py` script`
        self.all_keys = [
            "rs_time",
            "rs_color",
            "rs_depth",
            "pred_keypoints_3d",
            "pred_keypoints_2d",
            "wrist",
            "glove",
            "tmesh",
        ]

        # repetitive from BowieSyncData class, want these to confirm matching
        self.data_stats = [
            [
                "Filename",
                "Realsense Frames",
                "Hand Frames",
                "No Hand Frames",
                "Hand Error Loss",
                "Bowie Frames",
                "Key Error Frames",
                "Key Error Loss",
            ]
        ]
        self.error_stats = [["Filename", "Key Error Frames"]]
        self.sensor_ids = None

        # wrist frames are different on hand model and robot
        self.hand_to_robot_matrix = np.array(
            [[-0, -1, -0, 0], [0, 0, 1, 0], [-1, -0, -0, 0], [0, 0, 0, 1]]
        )
        self.h2r_matrix = np.array(
            [[0, 0, -1, 0], [-1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1]]
        )

        # 1. find available data
        self.filenames, self.filepaths = self._get_extracted_filenames()
        # self.filenames = self.filenames[:1]
        # self.filepaths = self.filepaths[:1]
        # 2. read in the data
        # 2A. check for any keyerrors on valid sensor_ids, fill it in with 0s
        self.data = {}
        self._load_data()
        self._clean_data()

        # 3. use wrist estimates to generate arm joints (7)
        # 3A. wrist pose in camera frame
        # 3B. wrist pose in initial wrist frame
        # 3C. wrist pose in X frame rotated for robot hand
        # 3D. retarget

        self.franka_chain, self.franka_limits = self._get_kinematic_chains(
            self.cfg.franka_ik
        )
        # todo this should be wrapped inside self.franka_kinematics
        initial_ee_pose = self.franka_chain.forward_kinematics(
            np.array(cfg.franka_ik.initial_joints), end_only=True
        )
        self.T_base_toolinitial = initial_ee_pose.get_matrix()[0]
        self.T_base_toolinitial = np.array(self.T_base_toolinitial)
        print(f"T_base_toolinitial: \n{self.T_base_toolinitial}")

        self._transforms()
        # generate ee_poses from human and initial robot pose

        # generate joint states from the ee_poses with IK
        self.urdf_path = self.cfg.franka_ik.urdf_path
        self.device = self.cfg.device
        

        self._filter_keypoints()
        if self.cfg.retarget:
            self.franka_kinematics = FrankaKinematics(self.cfg.franka_ik)
            self.metahand_kinematics = DifferentiableMetahandKinematics(
            urdf_path=self.urdf_path, device=self.device, config=self.cfg.metahand_ik)

            self._retarget()
            self._filter_joint_states()

        if self.cfg.overwrite:
            self._overwrite_data()

    def _get_kinematic_chains(self, cfg):
        assert (
            cfg.urdf_path is not None
        ), "urdf_path needs to be specified in cfg.retarget"
        # share_dir = get_package_share_directory("meta_hand_description")
        # urdf_path = f"{share_dir}/urdf/meta_hand_franka.urdf"
        full_chain = pk.build_chain_from_urdf(open(cfg.urdf_path, mode="rb").read())

        franka_chain = pk.SerialChain(full_chain, "hand_base_link", "base_link")
        limits = franka_chain.get_joint_limits()
        if self.verbose:
            franka_chain.print_tree()
            print(f"Joints: {franka_chain.get_joint_parameter_names()}")
            print(f"Joint Limits: {franka_chain.get_joint_limits()}")
        return franka_chain.to(device=cfg.device), limits

    def _filter_keypoints(self):

        for filename in self.data.keys():
            kp = self.data[filename]["keypoints_wrist"]
            kp = self._apply_savgol_filter(kp)
            self.data[filename]["keypoints_wrist"] = kp

    def _filter_joint_states(self):

        for filename in self.data.keys():
            js = self.data[filename]["joint_states"]
            js = self._apply_savgol_filter(js)
            self.data[filename]["joint_states"] = js

    def _transforms(self):

        # for all the files, add "wrist_init"
        for filename in self.data.keys():
            data_dict = self.data[filename]

            # rotating wrist frame into allegro frame
            wrist_pose = data_dict["wrist"]
            robot_pose = np.copy(wrist_pose)
            for frame in range(wrist_pose.shape[0]):
                robot_pose[frame] = (
                    np.linalg.inv(wrist_pose[frame]) @ robot_pose[frame]
                )  # bring wrist pose into reference frame
                robot_pose[frame] = (
                    self.h2r_matrix @ robot_pose[frame]
                )  # rotate into robot frame
                robot_pose[frame] = (
                    wrist_pose[frame] @ robot_pose[frame]
                )  # bring back into camera frame
            self.data[filename]["robot"] = robot_pose

            # wrist transforms, from camera frame to initial and base frames
            self.data[filename]["robot_init"], self.data[filename]["robot_base"] = (
                self._robot_cam_to_init(self.data[filename]["robot"])
            )

            # keypoints transforms, from camera frame to initial and base frames
            (
                self.data[filename][
                    "keypoints_init"
                ],  # relative to initial wrist frame
                self.data[filename]["keypoints_wrist"],  # relative to wrist frame
                self.data[filename]["keypoints_base"],  # relative to robot base frame
            ) = self._keyp_cam_to_robot(
                self.data[filename]["robot"], self.data[filename]["pred_keypoints_3d"]
            )

    def plot_pose_trajectory(self, filename, frame="wrist"):
        """
        filename = one of the filenames in self.filenames
        frame = "camera", "initial", or "delta" see docs for explanation TODO
        """
        # assert key in data dict
        fig = self._plot_origin_frame()
        if frame == "wrist":
            self._add_wrist_trajectory_to_plot(fig, self.data[filename]["wrist"])
        elif frame == "robot_init":
            self._add_wrist_trajectory_to_plot(fig, self.data[filename]["robot_init"])
        elif frame == "robot_base":
            self._add_wrist_trajectory_to_plot(fig, self.data[filename]["robot_base"])
        elif frame == "delta":
            raise NotImplementedError
        else:
            raise NotImplementedError
        return fig

    def _add_wrist_trajectory_to_plot(self, fig, wrist_trajectory):
        """
        wrist_trajectory should be a np.array of (NUM_FRAMES, 4, 4)
        """
        # print(wrist_trajectory[0, :, :])
        wrist_orientation = wrist_trajectory[:, :3, :3]
        wrist_position = wrist_trajectory[:, :3, -1]

        gradient_color = np.linspace(0, 1, len(wrist_position[:, 0]))
        fig.add_traces(
            [
                go.Scatter3d(
                    x=wrist_position[:, 0],
                    y=wrist_position[:, 1],
                    z=wrist_position[:, 2],
                    mode="markers",
                    marker=dict(
                        size=1,
                        color=gradient_color,
                        colorscale="RdYlGn",
                        reversescale=True,
                        showscale=False,  # True if you want the scale bar
                    ),
                ),
                go.Scatter3d(
                    x=wrist_position[:1, 0],
                    y=wrist_position[:1, 1],
                    z=wrist_position[:1, 2],
                    mode="markers",
                    marker=dict(size=5, color="green", symbol="cross"),
                ),  # starting point
                go.Scatter3d(
                    x=wrist_position[-1:, 0],
                    y=wrist_position[-1:, 1],
                    z=wrist_position[-1:, 2],
                    mode="markers",
                    marker=dict(size=2, color="red", symbol="x"),
                ),  # end point
            ]
        )
        # print(wrist_orientation[0])
        # self._add_wrist_frame(fig, wrist_pose)
        self._add_wrist_frame(fig, wrist_trajectory[0])

    def _keyp_cam_to_robot(self, robot_cam, keyp_cam):
        """
        input:
        robot cam: np.array of (N,4,4) transforms in the camera frame
        keyp_cam: np.array of keypoints in camera frame (NUM_FRAMES, 21, 3)

        Example: self.data[filename]["robot"], self.data[filename]["pred_keypoints_3d"]
        """
        T_cam_robot0 = robot_cam[0]
        keypoints_rel_wrist = np.copy(keyp_cam)  # (N, 21, 3)
        keypoints_rel_init = np.copy(keyp_cam)  # (N, 21, 3)
        keypoints_rel_base = np.copy(keyp_cam)  # (N, 21, 3)

        # TODO I can speed this up with vectorization or einsum
        # for now this is clearer for debugging TLH
        for frame in range(keyp_cam.shape[0]):
            keypoints = keyp_cam[frame]  # (21, 3)
            hom_keypoints = np.hstack((keypoints, np.ones((21, 1))))  # (21, 4)

            # TODO add keypoints relative to each wrist frame
            keyp_wrist = np.linalg.inv(robot_cam[frame]) @ hom_keypoints.T
            keypoints_rel_wrist[frame] = keyp_wrist[:-1, :].T  # drop homogenous coords

            # (4, 4) @ (4, 21) -> (21, 3)
            keyp_init = np.linalg.inv(T_cam_robot0) @ hom_keypoints.T
            keypoints_rel_init[frame] = keyp_init[:-1, :].T  # drop homogenous coords

            keyp_base = self.T_base_toolinitial @ keyp_init  # (4, 4) @ (4, 21)
            keypoints_rel_base[frame] = keyp_base[:-1, :].T  # drop homogenous coords

        return keypoints_rel_init, keypoints_rel_wrist, keypoints_rel_base

    def _robot_cam_to_init(self, robot_cam):
        """Transform robot wrist poses
        Args:
            robot_cam (np.array): (N, 4, 4) robot poses in camera frame
        Returns:
            robot_pose_rel_init (np.array): (N, 4, 4) robot poses in initial camera frame
            robot_pose_rel_base (np.array): (N, 4, 4) robot poses in robot base frame
        """
        T_cam_robot0 = robot_cam[0]
        robot_pose_rel_init = (
            np.linalg.inv(T_cam_robot0) @ robot_cam
        )  # (4,4) @ (N, 4, 4)
        robot_pose_rel_base = self.T_base_toolinitial @ robot_pose_rel_init

        return robot_pose_rel_init, robot_pose_rel_base

    def add_keypoints(self, fig, filename, key, frame=0):
        """Add keypoints from filename[key][frame] to fig
        Args:
            fig (Figure): plotly.graph_objects.Figure instance to add keypoints to
            filename (str): filename string from self.filenames
            key (str): one of four available keys: ["pred_keypoints_3d", "keypoints_wrist", "keypoints_init", "keypoints_base"]
            frame (int, optional): frame of keypoints you want to add to fig. Defaults to 0.
        """
        available_keys = [
            "pred_keypoints_3d",
            "keypoints_wrist",
            "keypoints_init",
            "keypoints_base",
        ]
        assert (
            key in available_keys
        ), f"Invalid key: '{key}'. Available keys are: {available_keys}"
        # pred_keypoints_3d
        #
        keypoints = self.data[filename][key]

        assert (
            0 <= frame < keypoints.shape[0]
        ), f"Frame {frame} out of range {filename} with length {keypoints.shape[0]}"
        keypoints = keypoints[frame]
        # print(f"keypoints = {keypoints}")
        fig.add_traces(
            data=[
                go.Scatter3d(
                    x=keypoints[:, 0],
                    y=keypoints[:, 1],
                    z=keypoints[:, 2],
                    mode="markers",
                    marker=dict(size=2, color="green"),
                ),
            ]
        )

    def _add_wrist_frame(self, fig, wrist_pose):
        """Add reference frame around a provided wrist pose

        Args:
            fig (Figure): plotly.graph_objects.Figure instance add add wrist frame to
            wrist_pose (np.array): (4, 4) wrist pose matrix
        """
        scale = 0.1
        basis_points = np.array(
            [
                [0, 0, 0],
                [scale, 0, 0],
                [0, 0, 0],
                [0, scale, 0],
                [0, 0, 0],
                [0, 0, scale],
            ]
        )

        rotated_basis = basis_points @ wrist_pose[:3, :3].T + wrist_pose[:3, -1]
        print(rotated_basis)
        # rotated_start = start @ wrist_pose[:3, :3] + wrist_pose[:3, -1]
        # rotated_end = end @ wrist_pose[:3, :3] + wrist_pose[:3, -1]
        # print(start)
        # print(rotated_start)
        # print(end)
        # print(rotated_end)

        fig.add_traces(
            data=[
                go.Scatter3d(
                    x=[rotated_basis[0, 0], rotated_basis[1, 0]],
                    y=[rotated_basis[0, 1], rotated_basis[1, 1]],
                    z=[rotated_basis[0, 2], rotated_basis[1, 2]],
                    mode="lines",
                    line=dict(color="red", width=1),
                ),
                go.Scatter3d(
                    x=[rotated_basis[2, 0], rotated_basis[3, 0]],
                    y=[rotated_basis[2, 1], rotated_basis[3, 1]],
                    z=[rotated_basis[2, 2], rotated_basis[3, 2]],
                    mode="lines",
                    line=dict(color="green", width=1),
                ),
                go.Scatter3d(
                    x=[rotated_basis[4, 0], rotated_basis[5, 0]],
                    y=[rotated_basis[4, 1], rotated_basis[5, 1]],
                    z=[rotated_basis[4, 2], rotated_basis[5, 2]],
                    mode="lines",
                    line=dict(color="blue", width=1),
                ),
            ]
        )

    def _plot_origin_frame(self):
        """Create fig instance with only origin frame

        Returns:
            fig (Figure): plotly.graph_objects.Figure instance with origin frame
        """
        # todo add rotation and orientation inputs
        scale = 0.25
        fig = go.Figure()
        fig.add_traces(
            data=[
                go.Scatter3d(
                    x=[0, scale],
                    y=[0, 0],
                    z=[0, 0],
                    mode="lines",
                    line=dict(color="red", width=1),
                ),
                go.Scatter3d(
                    x=[0, 0],
                    y=[0, scale],
                    z=[0, 0],
                    mode="lines",
                    line=dict(color="green", width=1),
                ),
                go.Scatter3d(
                    x=[0, 0],
                    y=[0, 0],
                    z=[0, scale],
                    mode="lines",
                    line=dict(color="blue", width=1),
                ),
            ]
        )
        fig.update_layout(
            scene=dict(
                xaxis_title="X",
                yaxis_title="Y",
                zaxis_title="Z",
            ),
            scene_dragmode="orbit",
        )

        return fig

    def _retarget(self):
        """for each file in self.data, calculates input np.array (N,4,4) of target poses in robot base frame
        to joint states (N, 1, 7) using self.franka_ik and self.metahand_ik

        Raises:
            NotImplementedError: if the ik solver selected is not in ["default", "lbfgs"]
        """
        for filename in self.data.keys():
            robot_poses = self.data[filename]["robot_base"]

            target_position = torch.tensor(robot_poses[:, :3, -1], dtype=torch.float32)
            target_orientation = torch.tensor(
                robot_poses[:, :3, :3], dtype=torch.float32
            )

            franka_joints, target_pose = self.franka_kinematics.inverse_kinematics(
                target_positions=target_position,
                target_orientations=target_orientation,
            )  # both pos/rotation of ee matter
            # print(
            #     f"joint ik solutions: {franka_joints.shape}, {err_rot.shape}, {err_pos.shape}"
            # )
            if self.cfg.metahand_ik.ik_solver == "lbfgs":
                # TODO in testing, seed from first few frames not the whole trajectory
                keypoints = self.data[filename]["keypoints_wrist"]
                key = self._find_closest_grasp_key(keypoints)

                # set initial seed based on lookup dictionary
                self.metahand_kinematics.prev_joints = torch.tensor(
                    ALLEGRO_JOINTS[key], dtype=torch.float32
                )
                keypoints = torch.from_numpy(keypoints).float()
                allegro_joints = self.metahand_kinematics.inverse_kinematics_keypoints(
                    keypoints
                )
            else:
                raise NotImplementedError(
                    f"self.config.metahand_ik.ik_solver: {self.config.metahand_ik.ik_solver} not tested"
                )

            joint_states = torch.cat((franka_joints, allegro_joints), dim=2)
            self.data[filename]["joint_states"] = np.array(joint_states)

    def _find_closest_grasp_key(self, keypoints):
        """Given a set of keypoints, find the closest grasp key

        Args:
            keypoints (np.array):(21,3) array of keypoints

        Returns:
            key (str): key for closest grasp reference
        """
        return min(
            HAND_KEYPOINTS,
            key=lambda key: np.linalg.norm(keypoints - HAND_KEYPOINTS[key]),
        )

    def _apply_savgol_filter(self, data):
        """
        Apply Savitzky-Golay filter to the input data.
        Parameters:
        - data (array_like): Input data. Can be 1D, 2D, or 3D.
        - window_length (int): Length of the filter window. It must be an odd integer number.
        - polyorder (int): Order of the polynomial used to fit the samples.
        - axis (int, optional): Axis along which to apply the filter. Default is 0.
        Returns:
        - filtered_data (array_like): Filtered data.
        """
        assert len(data.shape) == 3, "Input data must be 3D"
        filtered_data = savgol_filter(data, self.window_length, self.polyorder, axis=0)
        # filtered_data = np.apply_along_axis(lambda x: savgol_filter(x, window_length, polyorder), axis=0, data)
        return filtered_data

    def print_stats(self):
        """
        prints any key or hand detection errors BEFORE data cleaning.
        """
        file_stats = tabulate([*self.data_stats])
        print(file_stats)

    def print_error_stats(self):
        """
        prints any key errors AFTER data cleaning. Errors should always be 0 for all files
        if not all 0, then a new error case has occurred and warrants investigation
        """
        file_stats = tabulate([*self.error_stats])
        print(file_stats)

    def _get_extracted_filenames(self):
        """Walks through self.cfg.paths.out_data_path and grabs any filenames that end with .pkl for processing
        Returns:
            filenames (List): list of filenames for *.pkl in self.cfg.paths.out_data_path
            filepaths (List): list of absolute filepaths for *.pkl in self.cfg.paths.out_data_path
        """
        #
        filenames = []
        filepaths = []
        for root, dirs, files in os.walk(self.cfg.paths.out_data_path):
            for file in files:
                filepath = os.path.join(root, file)
                if filepath.endswith(".pkl"):
                    filepaths.append(filepath)
                    filenames.append(file)

        if len(filenames) == 0:
            print(f"Can't find any files. Is {self.cfg.paths.out_data_path} correct?")

        filenames.sort()
        filepaths.sort()
        return filenames, filepaths

    def _load_data(self):

        for filename in self.filenames:
            filepath = os.path.join(self.cfg.paths.out_data_path, filename)
            with open(filepath, "rb") as f:
                if self.verbose:
                    print(f"Opening {filepath}...")
                data_dict = pickle.load(f)
                if self._all_keys_present(filename, data_dict):
                    self.data[filename] = data_dict
                    # add stats while we run through
                    total_frames, hand_frames, no_hand_frames = (
                        self._get_missing_hand_frames(data_dict)
                    )
                    total_data, total_key_errors = self._get_key_errors(data_dict)
                    self.data_stats.append(
                        [
                            filename,
                            total_frames,
                            hand_frames,
                            no_hand_frames,
                            round(
                                no_hand_frames / total_frames, 4
                            ),  # hand error percent loss
                            total_data,
                            total_key_errors,
                            round(total_key_errors / total_data, 4),
                        ]
                    )
                    if self.verbose:
                        print(f"{filepath} added!")

    def _overwrite_data(self):

        for filename in self.filenames:
            filepath = os.path.join(self.cfg.paths.out_data_path, filename)
            # print(f"Overwriting {filepath}...")
            data_dict = self.data[filename]
            # print(f"data dict keys: {data_dict.keys()}")

            with open(filepath, "wb") as f:
                pickle.dump(data_dict, f)

            print(f"{filepath} added!")

    def _all_keys_present(self, filename, data_dict):
        """
        pass in one files data_dict to check all expected keys are present
        return True if all keys are present
        return False if there are missing keys, additional warning message
        """
        missing_keys = [key for key in self.all_keys if key not in data_dict]
        if missing_keys:
            print(
                f"[WARNING] {filename} seems to be missing expected keys: {missing_keys}. Skipping file."
            )
            return False
        else:
            return True

    def _get_missing_hand_frames(self, data_dict):
        no_hand_frames = 0
        hand_objs = data_dict["tmesh"]
        for idx, _ in enumerate(hand_objs):
            if hand_objs[idx].is_empty:
                no_hand_frames += 1
        hand_frames = data_dict["rs_time"].shape[0]

        # print(data_dict.keys())
        total_frames = data_dict["rs_time"].shape[0]
        return total_frames, hand_frames, no_hand_frames

    def _get_key_errors(self, data_dict):

        bowie_data_array = data_dict["glove"]
        total_key_errors = 0
        self.sensor_ids = self._get_available_sensor_ids(bowie_data_array)
        for idx, sensor_id in enumerate(self.sensor_ids):
            if sensor_id:
                _, key_errors = self._get_sensor_key_errors(idx, bowie_data_array)
                total_key_errors += key_errors
                if self.verbose:
                    print(f"ID: {sensor_id} and KeyErrors: {key_errors}")
        return bowie_data_array.shape[0], total_key_errors

    def _get_available_sensor_ids(self, bowie_data_array):
        """
        input: bowie_data_array is a np.array of shape (FRAME, SENSOR_ID, 14)
        output: array of booleans indicating which sensors were present
        """
        # check if the mag time field is np.nan. if so, we know that sensor_id is not available in this recording.
        sensor_ids = ~np.isnan(bowie_data_array[0, :, 0])
        if self.verbose:
            for idx, item in enumerate(sensor_ids):
                if item:
                    print(f"Sensor ID {idx} found!")
        return sensor_ids

    def _get_sensor_key_errors(self, sensor_idx, bowie_data_array):
        assert 0 <= sensor_idx <= 40
        if sensor_idx < 20:
            nan_idxs = np.isnan(bowie_data_array[:, sensor_idx, :-1])
            num_nans = nan_idxs.sum()
        elif sensor_idx < 40:
            nan_idxs = np.isnan(bowie_data_array[:, sensor_idx, 0:6])
            num_nans = nan_idxs.sum()  # no quat
        else:
            return None
        return nan_idxs, num_nans

    def _clean_data(self):
        # self.data is a dictionary where the first key is a filename, and second key is the data_dict format
        # update glove data in self.data[1]['glove'] to fill in key errors
        for filename in self.data.keys():
            data_dict = self.data[filename]

            data_dict["glove"] = self._clean_bowie_array(data_dict["glove"])
            total_data, total_key_errors = self._get_key_errors(data_dict)

            self.error_stats.append(
                [
                    filename,
                    total_key_errors,
                ]
            )

    def _clean_bowie_array(self, bowie_data_array):
        # input: data_dict['glove'] holds (NUM_FRAMES, 40, 14) array of bowie glove data with keyerrors
        # output: (NUM_FRAMES, 40, 14) np.array without keyerrors

        assert self.sensor_ids is not None
        for idx, sensor_id in enumerate(self.sensor_ids):
            if sensor_id:
                assert 0 <= idx <= 40
                key_error_idxs, _ = self._get_sensor_key_errors(idx, bowie_data_array)
                if idx < 20:
                    bowie_data_array[:, idx, :-1][key_error_idxs] = 0
                elif idx < 40:
                    bowie_data_array[:, idx, 0:6][key_error_idxs] = 0
                else:
                    raise Exception

        return bowie_data_array
    
    ### helpers to generate visuals based on selected filename
    def export_realsense_gif(self, filename, fskip=2, interval=0.033, fps=30):
        """
        helper function to generate .gif from realsense frames
        ---
        fskip: downsample factor for animating the frames. fskip=2 skips every other frame
        interval: the timing for each frame on the animation
        fps: the final output fps
        """
        # grab rs_color data from provided filename
        assert filename in self.filenames, f"{filename} not found in 'self.filenames'"
        rs_color = self.data[filename]['rs_color']

        fig, ax = plt.subplots()
        def animate_color(i, fskip):
            ax.clear()
            ax.imshow(rs_color[i * fskip])
            ax.set_title(f"Frame {i*fskip}")

        anim = FuncAnimation(
            fig,
            animate_color,
            fargs=[fskip],
            frames=rs_color.shape[0] // fskip,
            interval=interval,
        )
        # TODO add output path location
        cwd = os.getcwd()
        output_filename = filename.replace(".pkl", ".gif")
        output_path = os.path.join(cwd, output_filename)
        anim.save(output_filename, writer="pillow", fps=30)
        print(f"realsense gif saved to {output_path}")
        return output_path

    def plot_all_raw_data(self, filename):
        assert filename in self.filenames, f"{filename} not found in 'self.filenames'"
        bowie_data_array = self.data[filename]['glove']

        for idx, item in enumerate(self.sensor_ids):
            if item:
                if idx < 20:
                    d = bowie_data_array[:, idx, :]
                    ref = bowie_data_array[:, idx + 20, :]
                    fig = plt.figure()
                    fig, ax = plt.subplots(1, 2)

                    ax[0].plot(d[:, 0], d[:, 3], "r")
                    ax[0].plot(d[:, 0], d[:, 4], "g")
                    ax[0].plot(d[:, 0], d[:, 5], "b")

                    ax[0].plot(ref[:, 0], ref[:, 3], "r--")
                    ax[0].plot(ref[:, 0], ref[:, 4], "g--")
                    ax[0].plot(ref[:, 0], ref[:, 5], "b--")

                    # diff = d[:,3:6]-ref[:,3:6]

                    # ax[0].plot(diff[:,0]-diff[0,0],'r')
                    # ax[0].plot(diff[:,1]-diff[0,1],'g')
                    # ax[0].plot(diff[:,2]-diff[0,2],'b')

                    ax[0].set_title(f"sensor {idx} mag")
                    ax[0].legend(["x", "y", "z", "ref_x", "ref_y", "ref_z"])

                    ax[1].plot(d[:, 0], d[:, 9], "r")
                    ax[1].plot(d[:, 0], d[:, 10], "g")
                    ax[1].plot(d[:, 0], d[:, 11], "b")
                    ax[1].plot(d[:, 0], d[:, 12], "k")

                    ax[1].set_title(f"sensor {idx} quat")
                    ax[1].legend(["x", "y", "z", "w"])

                    plt.suptitle(filename)
                    plt.show()
        return fig

## TODO check np.nan vs None to count (no sensor) vs (keyerror) tracking
class BowieSyncData:

    def __init__(self, filename, DATA_PATH, verbose=False):

        self.DATA_PATH = DATA_PATH
        self.filename = filename
        self.PATH = os.path.join(DATA_PATH, filename)
        self.verbose = verbose

        (realsense_data, bowie_data) = self.open_bowie_sync_pkl(self.PATH, self.verbose)
        self.realsense_data = realsense_data
        self.bowie_data = bowie_data

        (rs_time, rs_color, rs_depth, bowie_data_array) = self.get_bowie_sync_data(
            self.realsense_data, self.bowie_data, verbose=self.verbose
        )
        self.rs_time = rs_time
        self.rs_color = rs_color
        self.rs_depth = rs_depth
        # TODO finalize where I want to handle keyerrors.
        self.bowie_data_array = bowie_data_array

        self.threshold = 20

        sensor_ids = self.get_available_sensor_ids(self.bowie_data_array)
        self.sensor_ids = sensor_ids.tolist()

        self.total_key_errors = self.get_total_key_errors()
        # todo check None in list (no sensor present)

    def get_sensor_key_errors(self, sensor_idx):
        assert 0 <= sensor_idx <= 40
        if sensor_idx < 20:
            num_nans = np.isnan(self.bowie_data_array[:, sensor_idx, :-1]).sum()
            # the -1 column is quaternion accuracy, which does not need to be counted in key errors
        elif sensor_idx < 40:
            num_nans = np.isnan(
                self.bowie_data_array[:, sensor_idx, 0:6]
            ).sum()  # no quat
        else:
            return None
        return num_nans

    def get_total_key_errors(self):
        total = 0
        for id, flag in enumerate(self.sensor_ids):
            if flag:
                total += self.get_sensor_key_errors(id)

        if self.verbose:
            print(f"Total Key Errors: {total}")
        return total

    def open_bowie_sync_pkl(self, path, verbose=False):
        """
        input: path to .pkl file that was collected using sync script
        verbose: optional flag to see some information about the data printed to console
        output: realsense and bowie data as lists
        """
        with open(path, "rb") as f:
            data = pickle.load(f)

        (realsense_data, bowie_data) = data

        if verbose:
            print(f"Number of Realsense Frames: {len(realsense_data)}")
            print(f"Number of Bowie Frames: {len(bowie_data)}")

        return realsense_data, bowie_data

    def get_bowie_sync_data(self, realsense_data, bowie_data, verbose=False):
        """
        input: (realsense_data, bowie_data) tuple that was read from open_bowie_sync_pkl
        verbose: optional flag to see some information about the data printed to console
        output: realsense and bowie data in np.array
        """

        assert len(realsense_data[0]) == 3
        rs_time, rs_color, rs_depth = (
            [item[0] for item in realsense_data],
            [item[1] for item in realsense_data],
            [item[2] for item in realsense_data],
        )

        rs_time = np.array(rs_time)
        rs_color = np.array(rs_color)
        # TODO; channel mismatch between hamer and matplotlib
        # rs_color[:, :, :, [0, 2]] = rs_color[:, :, :, [2, 0]]  # BGR TO RGB
        rs_depth = np.array(rs_depth)

        bowie_mag_data, bowie_quat_data = (
            [item[0] for item in bowie_data],
            [item[1] for item in bowie_data],
        )

        bowie_data_array = self.bowie_sync_to_array(bowie_mag_data, bowie_quat_data)

        if verbose:
            # print(f"Length of Realsense Time: {len(rs_time)}")
            # print(f"Length of Realsense Color: {len(rs_color)}")
            # print(f"Length of Realsense Image: {len(rs_depth)}")
            print(f"Shape of Realsense Time: {(rs_time).shape}")
            print(f"Shape of Realsense Color: {(rs_color).shape}")
            print(f"Shape of Realsense Depth: {(rs_depth).shape}")
            print(f"Shape of Bowie Data: {bowie_data_array.shape}")

        return rs_time, rs_color, rs_depth, bowie_data_array

    def get_available_sensor_ids(self, bowie_data_array, verbose=False):
        """
        input: bowie_data_array is a np.array of shape (FRAME, SENSOR_ID, 14)
        output: array of booleans indicating which sensors were present
        """
        sensor_ids = ~np.isnan(bowie_data_array[0, :, 0])
        if verbose:
            for idx, item in enumerate(sensor_ids):
                if item:
                    print(f"Sensor ID {idx} found!")
        return sensor_ids

    def bowie_sync_to_array(self, bowie_mag_data, bowie_quat_data):
        """
        bowie_mag_data: is a list of lists of tuple (time, dict)
        bowie_quat_data: is a list of lists of tupe (time, dict)
        output: np.array of shape (num_frames, sensor_id-1, data)
                data is np.array of shape (14) where the columns are:
                [mcu_time, mag_sec, mag_ns, mag_x, mag_y, mag_z ...
                mcu_time, quat_time_sec, quat_sec, quat_ns, quat_x, quat_y, quat_z, quat_acc]
        # TODO confirm these timestamps values with firmware - TLH
        """
        # bowie_mag_data is a list where [FRAME, sensor_id, time, data_dict]
        assert len(bowie_mag_data) == len(bowie_quat_data)
        data = np.full((len(bowie_mag_data), len(bowie_mag_data[0]), 14), np.nan)
        for f, frame in enumerate(bowie_mag_data):
            for s, sensor in enumerate(frame):
                if sensor[0] is not None:
                    if sensor[1].get("mag"):  # mag key check
                        data[f, s, 0] = sensor[0]  # mcu_time
                        data[f, s, 1] = (
                            sensor[1].get("mag").get("seconds")
                        )  # chip_time seconds
                        data[f, s, 2] = (
                            sensor[1].get("mag").get("nanoseconds")
                        )  # chip_time nanoseconds
                        data[f, s, 3] = sensor[1].get("mag").get("x")  # mag_x
                        data[f, s, 4] = sensor[1].get("mag").get("y")  # mag_y
                        data[f, s, 5] = sensor[1].get("mag").get("z")  # mag_z

        for f, frame in enumerate(bowie_quat_data):
            for s, sensor in enumerate(frame):
                if sensor[0] is not None:
                    if sensor[1].get("quat"):  # quat key check
                        data[f, s, 6] = sensor[0]  # mcu_time
                        data[f, s, 7] = sensor[1].get("quat").get("seconds")
                        # chip_time seconds
                        data[f, s, 8] = sensor[1].get("quat").get("nanoseconds")
                        # chip_time nanoseconds
                        data[f, s, 9] = sensor[1].get("quat").get("x")  # quat_x
                        data[f, s, 10] = sensor[1].get("quat").get("y")  # quat_y
                        data[f, s, 11] = sensor[1].get("quat").get("z")  # quat_z
                        data[f, s, 12] = sensor[1].get("quat").get("w")  # quat_w
                        data[f, s, 13] = sensor[1].get("quat").get("accuracy")
                        # accuracy of quat calibration [1-4]

        return data

    def export_realsense_gif(self, fskip=2, interval=0.033, fps=30):
        """
        helper function to generate .gif from realsense frames
        ---
        fskip: downsample factor for animating the frames. fskip=2 skips every other frame
        interval: the timing for each frame on the animation
        fps: the final output fps
        """
        fig, ax = plt.subplots()

        def animate_color(i, fskip):
            ax.clear()
            ax.imshow(self.rs_color[i * fskip])
            ax.set_title(f"Frame {i*fskip}")

        anim = FuncAnimation(
            fig,
            animate_color,
            fargs=[fskip],
            frames=self.rs_color.shape[0] // fskip,
            interval=interval,
        )
        # TODO add output path location
        output_filename = self.filename.replace(".pkl", ".gif")
        anim.save(output_filename, writer="ffmpeg", fps=30)
        return output_filename

    def export_realsense_bowie_gif(self, fskip=2, interval=0.033, fps=30):
        """
        helper function to generate .gif from realsense and bowie RAW data
        ---
        fskip: downsample factor for animating the frames. fskip=2 skips every other frame
        interval: the timing for each frame on the animation
        fps: the final output fps. can adjust this to slow down or speed up the output gif
        """
        # fig, ax = plt.subplots()

        fig, ax1, ax2, ax3, line2, line3 = self.plot_realsense_bowie_data()
        ax2.set_ylim(-3500, 1500)
        ax3.set_ylim(-3500, 1500)

        def animate(i, fskip):
            idx = i * fskip
            ax1.clear()
            ax1.imshow(self.rs_color[idx])
            ax1.set_title(f"Frame {idx}")

            line2.set_xdata([idx, idx])
            ax2.set_xlim(idx - 75, idx + 75)

            line3.set_xdata([idx, idx])
            ax3.set_xlim(idx - 75, idx + 75)

        anim = FuncAnimation(
            fig,
            animate,
            fargs=[fskip],
            frames=600 // fskip,  # self.rs_color.shape[0] // fskip,
            interval=interval,
        )
        # TODO add output path location
        output_filename = self.filename.replace(".pkl", ".gif")
        anim.save(output_filename, writer="pillow", fps=30)
        return output_filename

    def plot_realsense_bowie_data(self):
        """
        shortcut for plotting 15 and 19 on the two finger glove
        """
        # Create a new figure
        fig = plt.figure(figsize=(10, 6))
        (ax1, ax2, ax3) = fig.subplots(1, 3)

        (line2,) = ax2.plot([0, 0], [-5000, 5000], "k")
        ax2.set_xlim(0, self.rs_color.shape[0])
        ax2.set_ylim(-3500, 3500)

        (line3,) = ax3.plot([0, 0], [-5000, 5000], "k")
        ax3.set_xlim(0, self.rs_color.shape[0])
        ax3.set_ylim(-3500, 3500)

        d = self.bowie_data_array[:, 15, :]
        ref = self.bowie_data_array[:, 35, :]
        ax2.plot(d[:, 3], "r")
        ax2.plot(d[:, 4], "g")
        ax2.plot(d[:, 5], "b")

        ax2.plot(ref[:, 3], "r--")
        ax2.plot(ref[:, 4], "g--")
        ax2.plot(ref[:, 5], "b--")

        ax2.set_xlabel("frame number")
        ax2.set_title(f"index mag")

        d = self.bowie_data_array[:, 19, :]
        ref = self.bowie_data_array[:, 39, :]
        ax3.plot(d[:, 3], "r")
        ax3.plot(d[:, 4], "g")
        ax3.plot(d[:, 5], "b")

        ax3.plot(ref[:, 3], "r--")
        ax3.plot(ref[:, 4], "g--")
        ax3.plot(ref[:, 5], "b--")
        ax3.set_xlabel("frame number")
        ax3.set_title(f"thumb mag")
        return fig, ax1, ax2, ax3, line2, line3

    def export_realsense_binary_gif(self, fskip, interval=1 / 30.0, fps=30):
        """
        helper function to generate .gif from realsense and bowie BINARY data
        ---
        fskip: downsample factor for animating the frames. fskip=2 skips every other frame
        interval: the timing for each frame on the animation
        fps: the final output fps. can adjust this to slow down or speed up the output gif
        """

        fig = plt.figure()
        # plt.clf()
        fig, ax = plt.subplots(1, 2, figsize=(6, 3))
        # plt.suptitle(self.filename)
        # ASSUMES SENSOR 15 and 19 for now 12.9.24 TLH
        (xyz_l1, xyz_l2, binary_l1, binary_l2) = self.plot_norms_binary(
            19, threshold=self.threshold
        )
        ax[1].plot(binary_l2 + 4, "c")  # thumb

        (xyz_l1, xyz_l2, binary_l1, binary_l2) = self.plot_norms_binary(
            15, threshold=self.threshold
        )
        ax[1].plot(binary_l2 + 2, "m")  # index

        ax[1].set_title(f"binary")
        ax[1].legend(["thumb", "index"], loc="lower left")

        # vertical line for frame number
        (line2,) = ax[1].plot([0, 0], [-5000, 5000], "k")
        ax[1].set_xlim(0, self.rs_color.shape[0])
        ax[1].set_ylim([-1, 6])

        def animate(i, fskip):
            idx = i * fskip
            ax[0].clear()
            ax[0].imshow(self.rs_color[idx])
            ax[0].set_title(f"Frame {idx}")

            line2.set_xdata([idx, idx])
            ax[1].set_xlim(idx - 75, idx + 75)

        anim = FuncAnimation(
            fig,
            animate,
            fargs=[fskip],
            frames=self.rs_color.shape[0] // fskip,
            interval=interval,
        )
        # TODO add output path location
        output_filename = self.filename.replace(".pkl", ".gif")
        anim.save(output_filename, writer="pillow", fps=fps)
        return output_filename

    def plot_all_raw_data(self):
        for idx, item in enumerate(self.sensor_ids):
            if item:
                if idx < 20:
                    d = self.bowie_data_array[:, idx, :]
                    ref = self.bowie_data_array[:, idx + 20, :]
                    fig = plt.figure()
                    fig, ax = plt.subplots(1, 2)

                    ax[0].plot(d[:, 0], d[:, 3], "r")
                    ax[0].plot(d[:, 0], d[:, 4], "g")
                    ax[0].plot(d[:, 0], d[:, 5], "b")

                    ax[0].plot(ref[:, 0], ref[:, 3], "r--")
                    ax[0].plot(ref[:, 0], ref[:, 4], "g--")
                    ax[0].plot(ref[:, 0], ref[:, 5], "b--")

                    # diff = d[:,3:6]-ref[:,3:6]

                    # ax[0].plot(diff[:,0]-diff[0,0],'r')
                    # ax[0].plot(diff[:,1]-diff[0,1],'g')
                    # ax[0].plot(diff[:,2]-diff[0,2],'b')

                    ax[0].set_title(f"sensor {idx} mag")
                    ax[0].legend(["x", "y", "z", "ref_x", "ref_y", "ref_z"])

                    ax[1].plot(d[:, 0], d[:, 9], "r")
                    ax[1].plot(d[:, 0], d[:, 10], "g")
                    ax[1].plot(d[:, 0], d[:, 11], "b")
                    ax[1].plot(d[:, 0], d[:, 12], "k")

                    ax[1].set_title(f"sensor {idx} quat")
                    ax[1].legend(["x", "y", "z", "w"])

                    plt.suptitle(self.filename)
                    plt.show()

    def plot_norms_binary(self, sensor_idx, threshold, plot=False):

        assert sensor_idx < 20
        # assert mode in ["l1", "l2"]
        d = self.bowie_data_array[:, sensor_idx, :]
        ref = self.bowie_data_array[:, sensor_idx + 20, :]

        # remove reference signal and then subtract t=0
        diff = d[:, 3:6] - ref[:, 3:6]
        diff = diff[:, 0:3] - diff[0, 0:3]

        xyz_l1 = np.sum(np.abs(diff[:, 0:3]), axis=1)
        xyz_l2 = np.linalg.norm(diff[:, 0:3], axis=1)

        # if mode == "l1":
        #     binary = ((xyz_l1) > threshold).astype(int)
        # elif mode == "l2":
        #     binary = ((xyz_l2) > threshold).astype(int)

        binary_l1 = ((xyz_l1) > threshold).astype(int)
        binary_l2 = ((xyz_l2) > threshold).astype(int)

        if plot:
            fig = plt.figure(figsize=(3, 1))
            fig, ax = plt.subplots(1, 3)

            ax[0].plot(diff[:, 0], "r")
            ax[0].plot(diff[:, 1], "g")
            ax[0].plot(diff[:, 2], "b")
            ax[0].set_title("baseline")
            ax[0].set_ylabel("flux (uT)")

            ax[1].plot((xyz_l1), "c")
            ax[1].plot((xyz_l2), "m")
            ax[1].axhline(y=threshold, color="k", linestyle="--")
            ax[1].legend(["l1", "l2"])
            ax[1].set_title("XYZ norms")

            ax[2].plot(binary_l1, "c")
            ax[2].plot(binary_l2, "m")
            ax[2].set_title(f"T>{threshold}")
            # ax[2].set_ylabel("binary")

            plt.suptitle(f"sensor: {sensor_idx}, {self.filename}")
            fig.text(0.5, 0.02, "frame number", ha="center")
            plt.show()
        return (xyz_l1, xyz_l2, binary_l1, binary_l2)

    def plot_ema_binary(self, sensor_idx, threshold, history, alpha, plot=True):

        assert sensor_idx < 20
        d = self.bowie_data_array[:, sensor_idx, :]
        ref = self.bowie_data_array[:, sensor_idx + 20, :]

        diff = d[:, 3:6] - ref[:, 3:6]
        diff = diff[:, 0:3] - diff[0, 0:3]
        print(f"diff: {diff.shape}")

        xyz_l1 = np.sum((np.abs(diff[:, :])), axis=1)
        xyz_l2 = np.linalg.norm(diff, axis=1)
        print(f"xyz_l1: {xyz_l1.shape}")
        print(f"xyz_l2: {xyz_l2.shape}")

        x = diff[:, 0:3]  # x y z
        # x = diff[:,2]

        ema = self.ema(x, history, alpha)
        ema = np.abs(ema)
        print(f"ema: {ema.shape}")
        # ema = np.linalg.norm(ema, axis=0)
        # t = threshold
        # threshold = np.array([t, t, t])

        binary = (ema > threshold).astype(int)
        print(f"binary: {binary.shape}")

        if plot:
            fig = plt.figure()
            fig, ax = plt.subplots(1, 4)

            ax[0].plot((x[:, 0]), "r", alpha=0.5)
            ax[0].plot((ema[:, 0]), "k--", alpha=0.5)
            ax[0].axhline(y=threshold, color="k", linestyle="--")
            ax[0].set_title("ema x")

            ax[1].plot((x[:, 1]), "g", alpha=0.5)
            ax[1].plot((ema[:, 1]), "k--", alpha=0.5)
            ax[1].axhline(y=threshold, color="k", linestyle="--")
            ax[1].set_title("ema y")

            ax[2].plot((x[:, 2]), "b", alpha=0.5)
            ax[2].plot((ema[:, 2]), "k--", alpha=0.5)
            ax[2].set_title("ema z")
            ax[2].axhline(y=threshold, color="k", linestyle="--")

            ax[3].plot((binary[:, 0] + 4), "r")
            ax[3].plot((binary[:, 1] + 2), "g")
            ax[3].plot((binary[:, 2]), "b")
            ax[3].set_title("binary XYZ")
            ax[3].set_ylim([-1, 5])
            # ax[2].legend(["binary x", "binary y", "binary z"])
            plt.suptitle(f"sensor idx: {sensor_idx}, ema Î±={alpha}")
            plt.show()

        return ema

    def ema(self, input, history, alpha=0.2):
        """
        exponential moving average for mag data
        input: np.array of shape (M, 3)
        """
        assert input.shape[1] == 3
        ema = np.zeros((input.shape[0], input.shape[1]))
        sma = np.mean(input[:history, :], axis=0)
        ema[history - 1, :] = sma
        for i in range(history, input.shape[0]):
            # note: test ema for history window as well
            ema[i, :] = (input[i, :] * alpha) + (ema[i - 1, :] * (1 - alpha))
        return ema